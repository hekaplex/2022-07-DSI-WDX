{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15903\n",
      "Skipping records at range - 3780\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Working Copy  2022-02-25 0900AM\n",
    "#pip install BeautifulSoup as b  & #pip install requests\n",
    "\n",
    "from bs4 import BeautifulSoup as b\n",
    "import numpy as np\n",
    "import requests as r\n",
    "import pandas as p\n",
    "import math as m\n",
    "import csv as cv\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "# getting search days from user\n",
    "def getDates(txt):\n",
    "    msg1 = 'Enter the search ' + txt + ' in YYYY-MM-DD format'\n",
    "    startDate = input(msg1)\n",
    "    gDate = 0 \n",
    "\n",
    "    while gDate != 1:\n",
    "        try:\n",
    "            Syear, Smonth, Sday = map(int, startDate.split('-'))\n",
    "            date1 = date(Syear, Smonth, Sday)\n",
    "        except: \n",
    "            startDate = input(msg1)\n",
    "            continue\n",
    "        gDate = 1 \n",
    "    return date1\n",
    "date1 = getDates('startDate')\n",
    "date2 = getDates('endDate')\n",
    "\n",
    "#Get new search string\n",
    "base = date1\n",
    "stringA = \"https://www.fpds.gov/ezsearch/fpdsportal?q=+SIGNED_DATE%3A%5B\"\n",
    "base = date1\n",
    "endt = date2\n",
    "stringB = str(base.year)+\"%2F\"+base.strftime(\"%m\") + \"%2F\" + base.strftime(\"%d\") + \"%2C\"+str(endt.year)+\"%2F\" + endt.strftime(\"%m\") + \"%2F\" +endt.strftime(\"%d\") + \"%5D&s=FPDS.GOV&templateName=1.5.2&indexName=awardfull&x=30&start=\"\n",
    "stringC = '30'\n",
    "myURLsearch = (stringA + stringB + stringC)\n",
    "myURLsearch\n",
    "downloadURL = r.get(myURLsearch,headers={})\n",
    "soup = b(downloadURL.text,\"html.parser\")\n",
    "#CSS\n",
    "fulltable_select  = soup.select('span.results_heading')\n",
    "myLen = m.ceil(int(fulltable_select[1].find_all('b')[2].text))\n",
    "print(myLen)\n",
    "\n",
    "runInt = 1\n",
    "wID = 0\n",
    "while wID < myLen + 60:\n",
    "    #endt = base + timedelta(days=1)\n",
    "    stringB = str(base.year)+\"%2F\"+base.strftime(\"%m\") + \"%2F\" + base.strftime(\"%d\") + \"%2C\"+str(endt.year)+\"%2F\" + endt.strftime(\"%m\") + \"%2F\" +endt.strftime(\"%d\") + \"%5D&s=FPDS.GOV&templateName=1.5.2&indexName=awardfull&x=24&y=7&start=\"\n",
    "    stringC = int(stringC) + 30\n",
    "    stringC = str(stringC)\n",
    "    myURLsearch = (stringA + stringB + stringC)\n",
    "    downloadURL = r.get(myURLsearch,headers={})\n",
    "    soup = b(downloadURL.text,\"html.parser\")\n",
    "    fulltable_select  = soup.select('span.results_text')\n",
    "    datalist = []\n",
    "    slop =[]\n",
    "    for d in fulltable_select: \n",
    "        try: \n",
    "            datalist.append(d.a.contents)\n",
    "        except:\n",
    "            try:\n",
    "                datalist.append(d.span.contents)\n",
    "            except:\n",
    "                #formerly known as slop\n",
    "                datalist.append(str(d.contents).replace(\"\\\\t\",\"\").replace(\"\\\\n\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\'\",\"\"))\n",
    "            continue\n",
    "    \n",
    "    try:\n",
    "        newlist = [\"\".join(d) if type(d) is list else d for d in datalist ]\n",
    "        #try block as just-in-case the field return is not 19\n",
    "        \n",
    "        dlarr = np.array(newlist).reshape(m.floor(len(newlist)/19),19)\n",
    "            \n",
    "        #Get Dataframe\n",
    "        df = p.DataFrame(data= dlarr\n",
    "        , columns=['AwardID','AwardType','LegalBusinessName','ContractingAgency','DateSigned','ActionObligation','ReferencedIDV','ContractingOffice','NAICS_Code','PSC_Code','EntityCity','UniqueEntityID_DUNS', 'EntityState','UniqueEntityID_SAM','Zip','UltimateParentUniqueEntityID_DUNS','UltimateParentLegalBusinessName','UltimateParentUniqueEntityID_SAM','CAGE_Code']\n",
    "        )\n",
    "        r2 = datetime.today().strftime(\"%Y%m%d%H%M%S\")\n",
    "                \n",
    "        # Create run interval interger\n",
    "        if wID < 1:\n",
    "            fileName = 'fpds_ng_' + r2 + '.csv'\n",
    "            df.to_csv(fileName,index = False)\n",
    "            wID = wID + 30\n",
    "        else:\n",
    "            with open(fileName, \"a\", newline=\"\") as file:\n",
    "                writer = cv.writer(file,delimiter= \",\",)\n",
    "                writer.writerows(dlarr)\n",
    "            wID = wID + 30\n",
    "            \n",
    "    except:\n",
    "        print(\"Skipping records at range - \" + str(wID))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6870\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ca8c85afd634>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mmyURLsearch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstringA\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstringC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mdownloadURL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyURLsearch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownloadURL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mfulltable_select\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span.results_text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mdatalist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mParserRejectedMarkup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[1;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\bs4\\builder\\_htmlparser.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m             \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mHTMLParseError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             warnings.warn(RuntimeWarning(\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\html\\parser.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgoahead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\html\\parser.py\u001b[0m in \u001b[0;36mgoahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mmatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteresting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# < or &\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m                     \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for SQL\n",
    "#Working Copy  2022-02-25 0900AM\n",
    "#pip install BeautifulSoup as b  & #pip install requests\n",
    "\n",
    "from bs4 import BeautifulSoup as b\n",
    "import numpy as np\n",
    "import requests as r\n",
    "import pandas as p\n",
    "import math as m\n",
    "import csv as cv\n",
    "from datetime import date, timedelta, datetime\n",
    "stringA = \"https://www.fpds.gov/ezsearch/fpdsportal?indexName=awardfull&templateName=1.5.2&s=FPDS.GOV&q=+SIGNED_DATE%3A%5B2022%2F02%2F24%2C2022%2F02%2F24%5D&x=20&y=\"\n",
    "stringC = str(0)\n",
    "myURLsearch = (stringA + stringC)\n",
    "myURLsearch\n",
    "downloadURL = r.get(myURLsearch,headers={})\n",
    "soup = b(downloadURL.text,\"html.parser\")\n",
    "#CSS\n",
    "fulltable_select  = soup.select('span.results_heading')\n",
    "myLen = m.ceil(int(fulltable_select[1].find_all('b')[2].text))\n",
    "print(myLen)\n",
    "\n",
    "runInt = 1\n",
    "wID = 0\n",
    "while wID < myLen + 60:\n",
    "    stringC = int(stringC) + 30\n",
    "    myURLsearch = (stringA + str(stringC))\n",
    "    downloadURL = r.get(myURLsearch,headers={})\n",
    "    soup = b(downloadURL.text,\"html.parser\")\n",
    "    fulltable_select  = soup.select('span.results_text')\n",
    "    datalist = []\n",
    "    slop =[]\n",
    "    for d in fulltable_select: \n",
    "        try: \n",
    "            datalist.append(d.a.contents)\n",
    "        except:\n",
    "            try:\n",
    "                datalist.append(d.span.contents)\n",
    "            except:\n",
    "                #formerly known as slop\n",
    "                datalist.append(str(d.contents).replace(\"\\\\t\",\"\").replace(\"\\\\n\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\'\",\"\"))\n",
    "            continue\n",
    "    \n",
    "    try:\n",
    "        newlist = [\"\".join(d) if type(d) is list else d for d in datalist ]\n",
    "        #try block as just-in-case the field return is not 19\n",
    "        \n",
    "        dlarr = np.array(newlist).reshape(m.floor(len(newlist)/19),19)\n",
    "            \n",
    "        #Get Dataframe\n",
    "        df = p.DataFrame(data= dlarr\n",
    "        , columns=['AwardID','AwardType','LegalBusinessName','ContractingAgency','DateSigned','ActionObligation','ReferencedIDV','ContractingOffice','NAICS_Code','PSC_Code','EntityCity','UniqueEntityID_DUNS', 'EntityState','UniqueEntityID_SAM','Zip','UltimateParentUniqueEntityID_DUNS','UltimateParentLegalBusinessName','UltimateParentUniqueEntityID_SAM','CAGE_Code']\n",
    "        )\n",
    "        r2 = datetime.today().strftime(\"%Y%m%d%H%M%S\")\n",
    "                \n",
    "        # Create run interval interger\n",
    "        if wID < 1:\n",
    "            fileName = 'fpds_ng_' + r2 + '.csv'\n",
    "            df.to_csv(fileName,index = False)\n",
    "            wID = wID + 30\n",
    "        else:\n",
    "            with open(fileName, \"a\", newline=\"\") as file:\n",
    "                writer = cv.writer(file,delimiter= \",\",)\n",
    "                writer.writerows(dlarr)\n",
    "                wID = wID + 30\n",
    "                #print(wID)\n",
    "    except:\n",
    "        print(\"Skipping records at range - \" + str(wID))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6870\n"
     ]
    }
   ],
   "source": [
    "#Troubleshooting OS Error\n",
    "#Working Copy  2022-02-25 0900AM\n",
    "#pip install BeautifulSoup as b  & #pip install requests\n",
    "\n",
    "from bs4 import BeautifulSoup as b\n",
    "import numpy as np\n",
    "import requests as r\n",
    "import pandas as p\n",
    "import math as m\n",
    "import csv as cv\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "# getting search days from user\n",
    "def getDates(txt):\n",
    "    msg1 = 'Enter the search ' + txt + ' in YYYY-MM-DD format'\n",
    "    startDate = input(msg1)\n",
    "    gDate = 0 \n",
    "\n",
    "    while gDate != 1:\n",
    "        try:\n",
    "            Syear, Smonth, Sday = map(int, startDate.split('-'))\n",
    "            date1 = date(Syear, Smonth, Sday)\n",
    "        except: \n",
    "            startDate = input(msg1)\n",
    "            continue\n",
    "        gDate = 1 \n",
    "    return date1\n",
    "date1 = getDates('startDate')\n",
    "date2 = getDates('endDate')\n",
    "\n",
    "#Get new search string\n",
    "base = date1\n",
    "stringA = \"https://www.fpds.gov/ezsearch/fpdsportal?q=+SIGNED_DATE%3A%5B\"\n",
    "base = date1\n",
    "endt = date2\n",
    "stringB = str(base.year)+\"%2F\"+base.strftime(\"%m\") + \"%2F\" + base.strftime(\"%d\") + \"%2C\"+str(endt.year)+\"%2F\" + endt.strftime(\"%m\") + \"%2F\" +endt.strftime(\"%d\") + \"%5D&s=FPDS.GOV&templateName=1.5.2&indexName=awardfull&x=30&start=\"\n",
    "stringC = '30'\n",
    "myURLsearch = (stringA + stringB + stringC)\n",
    "myURLsearch\n",
    "downloadURL = r.get(myURLsearch,headers={})\n",
    "soup = b(downloadURL.text,\"html.parser\")\n",
    "#CSS\n",
    "fulltable_select  = soup.select('span.results_heading')\n",
    "myLen = m.ceil(int(fulltable_select[1].find_all('b')[2].text))\n",
    "print(myLen)\n",
    "\n",
    "runInt = 1\n",
    "wID = 0\n",
    "while wID < myLen + 60:\n",
    "    #endt = base + timedelta(days=1)\n",
    "    stringB = str(base.year)+\"%2F\"+base.strftime(\"%m\") + \"%2F\" + base.strftime(\"%d\") + \"%2C\"+str(endt.year)+\"%2F\" + endt.strftime(\"%m\") + \"%2F\" +endt.strftime(\"%d\") + \"%5D&s=FPDS.GOV&templateName=1.5.2&indexName=awardfull&x=24&y=7&start=\"\n",
    "    stringC = int(stringC) + 30\n",
    "    stringC = str(stringC)\n",
    "    myURLsearch = (stringA + stringB + stringC)\n",
    "    downloadURL = r.get(myURLsearch,headers={})\n",
    "    soup = b(downloadURL.text,\"html.parser\")\n",
    "    fulltable_select  = soup.select('span.results_text')\n",
    "    datalist = []\n",
    "    slop =[]\n",
    "    for d in fulltable_select: \n",
    "        try: \n",
    "            datalist.append(d.a.contents)\n",
    "        except:\n",
    "            try:\n",
    "                datalist.append(d.span.contents)\n",
    "            except:\n",
    "                #formerly known as slop\n",
    "                datalist.append(str(d.contents).replace(\"\\\\t\",\"\").replace(\"\\\\n\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\'\",\"\"))\n",
    "            continue\n",
    "    \n",
    "    try:\n",
    "        newlist = [\"\".join(d) if type(d) is list else d for d in datalist ]\n",
    "        #try block as just-in-case the field return is not 19\n",
    "        \n",
    "        dlarr = np.array(newlist).reshape(m.floor(len(newlist)/19),19)\n",
    "            \n",
    "        #Get Dataframe\n",
    "        df = p.DataFrame(data= dlarr\n",
    "        , columns=['AwardID','AwardType','LegalBusinessName','ContractingAgency','DateSigned','ActionObligation','ReferencedIDV','ContractingOffice','NAICS_Code','PSC_Code','EntityCity','UniqueEntityID_DUNS', 'EntityState','UniqueEntityID_SAM','Zip','UltimateParentUniqueEntityID_DUNS','UltimateParentLegalBusinessName','UltimateParentUniqueEntityID_SAM','CAGE_Code']\n",
    "        )\n",
    "        r2 = datetime.today().strftime(\"%Y%m%d%H%M%S\")\n",
    "                \n",
    "        # Create run interval interger\n",
    "        if wID < 1:\n",
    "            fileName = 'fpds_ng_' + r2 + '.csv'\n",
    "            df.to_csv(fileName,index = False)\n",
    "            wID = wID + 30\n",
    "        else:\n",
    "            with open(fileName, \"a\", newline=\"\") as file:\n",
    "                writer = cv.writer(file,delimiter= \",\",)\n",
    "                writer.writerows(dlarr)\n",
    "            wID = wID + 30\n",
    "            \n",
    "    except:\n",
    "        print(\"Skipping records at range - \" + str(wID))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable           Type             Data/Info\n",
      "---------------------------------------------\n",
      "b                  type             <class 'bs4.BeautifulSoup'>\n",
      "base               date             2022-02-24\n",
      "cv                 module           <module 'csv' from 'C:\\\\P<...>aconda3_64\\\\lib\\\\csv.py'>\n",
      "d                  Tag              <span class=\"results_text<...>25795\">25795</a>\\n</span>\n",
      "datalist           list             n=570\n",
      "date               type             <class 'datetime.date'>\n",
      "date1              date             2022-02-24\n",
      "date2              date             2022-02-24\n",
      "datetime           type             <class 'datetime.datetime'>\n",
      "df                 DataFrame           AwardID AwardType    L<...> DBQGN324ULK3     25795  \n",
      "dlarr              ndarray          30x19: 570 elems, type `<U35`, 79800 bytes\n",
      "downloadURL        Response         <Response [200]>\n",
      "endt               date             2022-02-24\n",
      "file               TextIOWrapper    <_io.TextIOWrapper name='<...>de='a' encoding='cp1252'>\n",
      "fileName           str              fpds_ng_20220301181940.csv\n",
      "fulltable_select   list             n=570\n",
      "getDates           function         <function getDates at 0x000002194CA040D0>\n",
      "m                  module           <module 'math' (built-in)>\n",
      "myLen              int              6870\n",
      "myURLsearch        str              https://www.fpds.gov/ezse<...>dfull&x=24&y=7&start=2070\n",
      "newlist            list             n=570\n",
      "np                 module           <module 'numpy' from 'C:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "os                 module           <module 'os' from 'C:\\\\Pr<...>naconda3_64\\\\lib\\\\os.py'>\n",
      "p                  module           <module 'pandas' from 'C:<...>es\\\\pandas\\\\__init__.py'>\n",
      "r                  module           <module 'requests' from '<...>\\\\requests\\\\__init__.py'>\n",
      "r2                 str              20220301182147\n",
      "runInt             int              1\n",
      "site               module           <module 'site' from 'C:\\\\<...>conda3_64\\\\lib\\\\site.py'>\n",
      "slop               list             n=0\n",
      "soup               BeautifulSoup    \\n<html><head><title>FPDS<...>\\n        }\\n}\\n</script>\n",
      "stringA            str              https://www.fpds.gov/ezse<...>rtal?q=+SIGNED_DATE%3A%5B\n",
      "stringB            str              2022%2F02%2F24%2C2022%2F0<...>awardfull&x=24&y=7&start=\n",
      "stringC            str              2070\n",
      "sys                module           <module 'sys' (built-in)>\n",
      "timedelta          type             <class 'datetime.timedelta'>\n",
      "wID                int              2010\n",
      "writer             writer           <_csv.writer object at 0x000002194FA7D990>\n"
     ]
    }
   ],
   "source": [
    "%whos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a82ef6f466d63b27a58f5b9b3bce23153a40d6708f87a46e08961f1e808d4b51"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
