{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis Pre-processing\n",
    "\n",
    "We will be using spaCy for data pre-processing and computational linguistics, gensim for topic modelling, scikit-learn for classification, and Keras for text generation.\n",
    "\n",
    "Based on ```https://github.com/bhargavvader/personal/tree/master/notebooks/text_analysis_tutorial```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 2.7.15 | packaged by conda-forge | (default, Feb 28 2019, 04:00:11) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import os, re, operator, warnings, sys\n",
    "warnings.filterwarnings('ignore')  # Let's not pay heed to them right now\n",
    "%matplotlib inline\n",
    "print('Python Version: %s' % (sys.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all extracted text files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() #get current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/storopoli/Documents/jupyter_notebooks/textract/INEP 2017d.txt\n",
      "/home/storopoli/Documents/jupyter_notebooks/textract/Trends 2018.txt\n",
      "/home/storopoli/Documents/jupyter_notebooks/textract/INEP 2017c.txt\n",
      "/home/storopoli/Documents/jupyter_notebooks/textract/Becker et al 2017.txt\n",
      "/home/storopoli/Documents/jupyter_notebooks/textract/OECD 2014.txt\n",
      "/home/storopoli/Documents/jupyter_notebooks/textract/Freeman, Becker _ Hall 2015.txt\n",
      "('Total Files:', 6)\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "for root, _, filenames in os.walk(path):\n",
    "     for filename in filenames:\n",
    "            if '.txt' in filename:\n",
    "                files.append(os.path.join(root, filename))\n",
    "\n",
    "for filename in files:\n",
    "     print filename\n",
    "print('Total Files:', len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to clean text\n",
    "We need to make sure to clean our data to make it unicode consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only if python 2 (not for python 3)\n",
    "def clean(text):\n",
    "    return unicode(''.join([i if ord(i) < 128 else ' ' for i in text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('Trends 2018.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data\n",
    "\n",
    "It's been often said in Machine Learning and NLP algorithms - garbage in, garbage out. We can't have state-of-the-art results without data which is aa good. Let's spend this section working on cleaning and understanding our data set. NTLK is usually a popular choice for pre-processing - but is a rather outdated and we will be checking out ```spaCy```, an industry grade text-processing package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English\n",
    "You need to run ```python -m spacy download en_core_web_sm``` in your environment before\n",
    "and then ```nlp = spacy.load('en_core_web_sm')```\n",
    "\n",
    "### Portuguese\n",
    "You need to run ```python -m spacy download pt_core_news_sm``` in your environment before\n",
    "and then ```nlp = spacy.load('pt_core_web_sm')```\n",
    "\n",
    "### Multi-laguage\n",
    "You need to run ```python -m spacy download xx_ent_wiki_sm``` in your environment before\n",
    "and then ```nlp = spacy.load('xx_ent_wiki_sm')```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Version: 2.1.4\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "print('spaCy Version: %s' % (spacy.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "```spaCy``` has some Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: [u'all', u'six', u'just', u'less', u'being', u'indeed', u'over', u'move', u'anyway', u'fifty']\n"
     ]
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(clean(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add some words to the stop word list\n",
    "texts, article = [], []\n",
    "for w in doc:\n",
    "    # if it's not a stop word or punctuation mark, add it to our article!\n",
    "    if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and w.text != 'I':\n",
    "        # we add the lematized version of the word\n",
    "        article.append(w.lemma_)\n",
    "    # if it's a new line, it means we're onto our next document\n",
    "    if w.text == '\\n':\n",
    "        texts.append(article)\n",
    "        article = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token.lemma_ for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to clean up text\n",
    "This functions tries to best pre-process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(text):\n",
    "    \"\"\"\n",
    "    This function clean up you text\n",
    "    and generate list of words for \n",
    "    each document.\n",
    "    \n",
    "    It also corrects for unicode problems\n",
    "    with python version 2.\n",
    "    \"\"\"\n",
    "    import sys, spacy\n",
    "    if sys.version_info.major == 2:\n",
    "        text = unicode(''.join([i if ord(i) < 128 else ' ' for i in text]))\n",
    "    removal=['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE']\n",
    "    text_out = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.is_stop == False and token.is_alpha and len(token)>2 and token.pos_ not in removal:\n",
    "            lemma = token.lemma_\n",
    "            text_out.append(lemma)\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_spacy(text):\n",
    "    \"\"\"\n",
    "    This function clean up you text\n",
    "    and generate list of words for \n",
    "    each document.\n",
    "    \n",
    "    It also corrects for unicode problems\n",
    "    with python version 2.\n",
    "    \"\"\"\n",
    "    import sys, spacy\n",
    "    if sys.version_info.major == 2:\n",
    "        text = unicode(''.join([i if ord(i) < 128 else ' ' for i in text]))\n",
    "    text_out = set()\n",
    "    clean = re.sub(\"\\s\\s+\",',',text)\n",
    "    clean =re.sub(\"'|•|<br/>\",\"\",clean)\n",
    "    clean =re.sub(r'\\w*(?=\\\\|:)','',clean)\n",
    "    text = re.sub(\"xa0|\\\\\\\\|:\",',',clean)\n",
    "    text = re.sub(\"(?<=\\w)\\s(?=\\w+\\,)\",'_',text)\n",
    "    text = re.sub(\"(?<=\\w)\\s(?=\\w+)\",'',text)\n",
    "    text = re.sub(r\"\\[|]\",' ',text)\n",
    "    text = ' '.join(text.split(','))\n",
    "\n",
    "    removal1= ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','VERB','ADJ','SYM','NOUN','X','NUM','SPACE']\n",
    "    doc= nlp(text)\n",
    "    for token in doc:\n",
    "        \n",
    "        if token.string == token.string.upper() and len(token)<15 and token.is_punct is False and token.is_alpha: \n",
    "            lemma = token.lemma_.strip() \n",
    "            text_out.add(lemma)            \n",
    "        if  token.pos_ not in removal1 and len(token)<15 and token.is_punct is False :\n",
    "            lemma = token.lemma_.strip()\n",
    "            if lemma != '':\n",
    "                text_out.add(lemma)\n",
    "    text_out = list(text_out)\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\n",
    "def cleanup_text(text, logging=False):\n",
    "    import sys, spacy\n",
    "    punctuations = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~©'\n",
    "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    if sys.version_info.major == 2:\n",
    "        text = unicode(''.join([i if ord(i) < 128 else ' ' for i in text]))\n",
    "    texts = []\n",
    "    doc = nlp(text, disable=['parser', 'ner'])\n",
    "    tokens = [token.lemma_.lower().strip() for token in doc if token.lemma_ != '-PRON-']\n",
    "    tokens = [token.lemma_ for token in tokens if token not in stopwords and token not in punctuations]\n",
    "    tokens = ' '.join(tokens)\n",
    "    texts.append(tokens)\n",
    "    return pd.Series(texts)\n",
    "#reviews['Description_Cleaned'] = reviews['description_Cleaned_1'].apply(lambda x: cleanup_text(x, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = clean_up(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp.txt', 'w') as file:\n",
    "    for line in temp:\n",
    "        file.write(line)\n",
    "        file.write('\\n')\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:textract]",
   "language": "python",
   "name": "conda-env-textract-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
