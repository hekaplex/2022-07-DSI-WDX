{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rbiKVIwfHxxS"
   },
   "source": [
    "# Introduction: Automated Machine Learning\n",
    "\n",
    "In this notebook, we will see how to use [TPOT](https://epistasislab.github.io/tpot/api/), a Python library developed for automatic machine learning feature preprocessing, model selection, and hyperparameter tuning. Using [genetic programming](http://geneticprogramming.com/tutorial/), TPOT tries to find the best machine learning pipeline for a dataset by evaluating thousands of possibilites. \n",
    "\n",
    "The machine learning pipeline in this context consists of:\n",
    "\n",
    "1. Feature Preprocessing\n",
    "  * Imputing missing values and scaling values\n",
    "  * Constructing new features such as polynomial transformations\n",
    "2. Feature selection\n",
    "  * Dimensionality reduction, for example using PCA and other techniques\n",
    "3. Model Selection\n",
    "  * Evaluting a number of machine learning models\n",
    "4. Hyperparameter tuning\n",
    "  * Finding the optimal settings of the model for the particular problem\n",
    "\n",
    "TPOT is one of a class of methods known as [auto-ml (short for automated machine learning)](https://www.kdnuggets.com/2017/01/current-state-automated-machine-learning.html) which aim to simplify the work of the data scientist by automatically finding the optimal (or near-optimal) feature preprocessing steps and model for the problem. Machine learning is  typically a very time-consuming and knowledge-intensive part of a data science problem. Auto-ml is not designed to replace the data scientist, but rather free her to work on more important aspects of the complete problem, such as acquiring data and interpreting the model results. In effect, TPOT, and auto-ml in general, will in effect be a \"data science assistant\" that will be another tool among many used by data scientists. Machine learning is only one part of the data science process, and it still takes a human to weave the different aspects of a problem together into a complete working product.\n",
    "\n",
    "Other entries in the field of auto - ml include:\n",
    "\n",
    "* [Auto-sklearn](https://automl.github.io/auto-sklearn/stable/)\n",
    "* [H20](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html)\n",
    "* [Google Cloud AutoML](https://cloud.google.com/automl/)\n",
    "\n",
    "\n",
    "With that background, let's see how automated machine learning, the future of data science, works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXV0OhoZylj8"
   },
   "source": [
    "First, because we are working in Google Colab, we need to make sure to install `TPOT`. We can do that using a system command (which in Jupyter is proceeded by `!`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "colab_type": "code",
    "id": "UX2YHrGwOuUN",
    "outputId": "1986ba7b-7068-47af-bc20-a223b346b78b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tpot\n",
      "  Using cached TPOT-0.11.7-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\andre\\anaconda3\\envs\\planutary\\lib\\site-packages (from tpot) (1.1.0)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in c:\\users\\andre\\anaconda3\\envs\\planutary\\lib\\site-packages (from tpot) (4.62.3)\n",
      "Requirement already satisfied: scipy>=1.3.1 in c:\\users\\andre\\anaconda3\\envs\\planutary\\lib\\site-packages (from tpot) (1.7.3)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\andre\\anaconda3\\envs\\planutary\\lib\\site-packages (from tpot) (1.3.5)\n",
      "Collecting stopit>=1.1.1\n",
      "  Using cached stopit-1.1.2.tar.gz (18 kB)\n",
      "Collecting xgboost>=1.1.0\n",
      "  Using cached xgboost-1.5.2-py3-none-win_amd64.whl (106.6 MB)\n",
      "Collecting deap>=1.2\n",
      "  Downloading deap-1.3.1-cp39-cp39-win_amd64.whl (108 kB)\n",
      "Collecting update-checker>=0.16\n",
      "  Using cached update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.16.3 in c:\\users\\andre\\anaconda3\\envs\\planutary\\lib\\site-packages (from tpot) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in c:\\users\\andre\\anaconda3\\envs\\planutary\\lib\\site-packages (from tpot) (1.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\andre\\anaconda3\\envs\\planutary\\lib\\site-packages (from pandas>=0.24.2->tpot) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\andre\\anaconda3\\envs\\planutary\\lib\\site-packages (from pandas>=0.24.2->tpot) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andre\\anaconda3\\envs\\planutary\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->tpot) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\andre\\anaconda3\\envs\\planutary\\lib\\site-packages (from scikit-learn>=0.22.0->tpot) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\anaconda3\\envs\\planutary\\lib\\site-packages (from tqdm>=4.36.1->tpot) (0.4.4)\n",
      "Requirement already satisfied: requests>=2.3.0 in c:\\users\\andre\\anaconda3\\envs\\planutary\\lib\\site-packages (from update-checker>=0.16->tpot) (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\andre\\anaconda3\\envs\\planutary\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\anaconda3\\envs\\planutary\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\andre\\anaconda3\\envs\\planutary\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\anaconda3\\envs\\planutary\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2021.10.8)\n",
      "Building wheels for collected packages: stopit\n",
      "  Building wheel for stopit (setup.py): started\n",
      "  Building wheel for stopit (setup.py): finished with status 'done'\n",
      "  Created wheel for stopit: filename=stopit-1.1.2-py3-none-any.whl size=11956 sha256=07391583b192c91996c51335a621dbe6957c082db4fb3211c9ab5e7e4e3947bd\n",
      "  Stored in directory: c:\\users\\andre\\appdata\\local\\pip\\cache\\wheels\\48\\8c\\93\\3afb1916772591fe6bcc25cdf8b1c5bdc362f0ec8e2f0fd413\n",
      "Successfully built stopit\n",
      "Installing collected packages: xgboost, update-checker, stopit, deap, tpot\n",
      "Successfully installed deap-1.3.1 stopit-1.1.2 tpot-0.11.7 update-checker-0.18.0 xgboost-1.5.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\andre\\anaconda3\\envs\\planutary\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Install tpot on the server\n",
    "!pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhBlHuGpO2n6"
   },
   "outputs": [],
   "source": [
    "# pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the tpot regressor\n",
    "from tpot import TPOTRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBJofhDZzuG3"
   },
   "source": [
    "# Problem Description\n",
    "\n",
    "The task is a supervised regression problem: given [New York City energy data](http://www.nyc.gov/html/gbee/html/plan/ll84_scores.shtml), we want to build a model that can predict the Energy Star Score of a building. In a series of articles ([part one](https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420), [part two](https://towardsdatascience.com/a-complete-machine-learning-project-walk-through-in-python-part-two-300f1f8147e2), [part three](https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-three-388834e8804b), [code on GitHub](https://github.com/WillKoehrsen/machine-learning-project-walkthrough)), we built a complete machine learning solution for this problem. Using manual feature engineering, dimensionality reduction, model selection, and hyperparameter tuning, we were able to build a model that achieved a mean absolute error of 9.06 points (on a scale of 1-100) on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dMni42UjIs75"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The features contain a number of continuous numeric variables (such as energy use and area of the building) as well as two one-hot encoded categorical variables (borough and building type). There are a total of 82 features. \n",
    "\n",
    "All of the missing values have been encoded as `np.nan`, and TPOT will automatically perform missing value imputation. It also automatically scales the variables so we do not have to worry about normalizing the range of each feature. TPOT does both feature engineering and feature selection, so we will not transform any of the variables or remove extraneous features we think may be extraneous. \n",
    "\n",
    "We will read into the data from GitHub and take a brief look.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "4O5zosPFO4U5",
    "outputId": "f9da185c-fb00-44e0-a682-53260c90ed50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape:  (6622, 82)\n",
      "Testing features shape:   (2839, 82)\n"
     ]
    }
   ],
   "source": [
    "# Read in features from GitHub\n",
    "train_features = pd.read_csv('https://raw.githubusercontent.com/WillKoehrsen/machine-learning-project-walkthrough/master/data/X_train.csv')\n",
    "test_features = pd.read_csv('https://raw.githubusercontent.com/WillKoehrsen/machine-learning-project-walkthrough/master/data/X_test.csv')\n",
    "\n",
    "# Read in labels from GitHub\n",
    "train_labels = pd.read_csv('https://raw.githubusercontent.com/WillKoehrsen/machine-learning-project-walkthrough/master/data/Y_train.csv')\n",
    "test_labels = pd.read_csv('https://raw.githubusercontent.com/WillKoehrsen/machine-learning-project-walkthrough/master/data/Y_test.csv')\n",
    "\n",
    "print('Training features shape: ', train_features.shape)\n",
    "print('Testing features shape:  ', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "colab_type": "code",
    "id": "Lrr9diIv1izQ",
    "outputId": "e03b50f9-8d4a-430f-8826-cce14d3f1661"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order</th>\n",
       "      <th>Property Id</th>\n",
       "      <th>DOF Gross Floor Area</th>\n",
       "      <th>Largest Property Use Type - Gross Floor Area (ft²)</th>\n",
       "      <th>Year Built</th>\n",
       "      <th>Number of Buildings - Self-reported</th>\n",
       "      <th>Occupancy</th>\n",
       "      <th>Site EUI (kBtu/ft²)</th>\n",
       "      <th>Weather Normalized Site EUI (kBtu/ft²)</th>\n",
       "      <th>Weather Normalized Site Electricity Intensity (kWh/ft²)</th>\n",
       "      <th>...</th>\n",
       "      <th>Largest Property Use Type_Restaurant</th>\n",
       "      <th>Largest Property Use Type_Retail Store</th>\n",
       "      <th>Largest Property Use Type_Self-Storage Facility</th>\n",
       "      <th>Largest Property Use Type_Senior Care Community</th>\n",
       "      <th>Largest Property Use Type_Social/Meeting Hall</th>\n",
       "      <th>Largest Property Use Type_Strip Mall</th>\n",
       "      <th>Largest Property Use Type_Supermarket/Grocery Store</th>\n",
       "      <th>Largest Property Use Type_Urgent Care/Clinic/Other Outpatient</th>\n",
       "      <th>Largest Property Use Type_Wholesale Club/Supercenter</th>\n",
       "      <th>Largest Property Use Type_Worship Facility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13276</td>\n",
       "      <td>5849784</td>\n",
       "      <td>90300.0</td>\n",
       "      <td>77300.0</td>\n",
       "      <td>1950</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>126.0</td>\n",
       "      <td>136.8</td>\n",
       "      <td>5.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7377</td>\n",
       "      <td>4398442</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>1926</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>95.4</td>\n",
       "      <td>102.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9479</td>\n",
       "      <td>4665374</td>\n",
       "      <td>104700.0</td>\n",
       "      <td>105000.0</td>\n",
       "      <td>1954</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>40.4</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14774</td>\n",
       "      <td>3393340</td>\n",
       "      <td>129333.0</td>\n",
       "      <td>129333.0</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>157.1</td>\n",
       "      <td>163.1</td>\n",
       "      <td>16.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3286</td>\n",
       "      <td>2704325</td>\n",
       "      <td>109896.0</td>\n",
       "      <td>116041.0</td>\n",
       "      <td>1927</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>62.3</td>\n",
       "      <td>68.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order  Property Id  DOF Gross Floor Area  \\\n",
       "0  13276      5849784               90300.0   \n",
       "1   7377      4398442               52000.0   \n",
       "2   9479      4665374              104700.0   \n",
       "3  14774      3393340              129333.0   \n",
       "4   3286      2704325              109896.0   \n",
       "\n",
       "   Largest Property Use Type - Gross Floor Area (ft²)  Year Built  \\\n",
       "0                                            77300.0         1950   \n",
       "1                                            52000.0         1926   \n",
       "2                                           105000.0         1954   \n",
       "3                                           129333.0         1992   \n",
       "4                                           116041.0         1927   \n",
       "\n",
       "   Number of Buildings - Self-reported  Occupancy  Site EUI (kBtu/ft²)  \\\n",
       "0                                    1        100                126.0   \n",
       "1                                    1        100                 95.4   \n",
       "2                                    1        100                 40.4   \n",
       "3                                    1        100                157.1   \n",
       "4                                    1        100                 62.3   \n",
       "\n",
       "   Weather Normalized Site EUI (kBtu/ft²)  \\\n",
       "0                                   136.8   \n",
       "1                                   102.0   \n",
       "2                                    40.0   \n",
       "3                                   163.1   \n",
       "4                                    68.2   \n",
       "\n",
       "   Weather Normalized Site Electricity Intensity (kWh/ft²)  ...  \\\n",
       "0                                                5.2        ...   \n",
       "1                                                4.7        ...   \n",
       "2                                                3.8        ...   \n",
       "3                                               16.9        ...   \n",
       "4                                                3.5        ...   \n",
       "\n",
       "   Largest Property Use Type_Restaurant  \\\n",
       "0                                     0   \n",
       "1                                     0   \n",
       "2                                     0   \n",
       "3                                     0   \n",
       "4                                     0   \n",
       "\n",
       "   Largest Property Use Type_Retail Store  \\\n",
       "0                                       0   \n",
       "1                                       0   \n",
       "2                                       0   \n",
       "3                                       0   \n",
       "4                                       0   \n",
       "\n",
       "   Largest Property Use Type_Self-Storage Facility  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                0   \n",
       "4                                                0   \n",
       "\n",
       "   Largest Property Use Type_Senior Care Community  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                1   \n",
       "4                                                0   \n",
       "\n",
       "   Largest Property Use Type_Social/Meeting Hall  \\\n",
       "0                                              0   \n",
       "1                                              0   \n",
       "2                                              0   \n",
       "3                                              0   \n",
       "4                                              0   \n",
       "\n",
       "   Largest Property Use Type_Strip Mall  \\\n",
       "0                                     0   \n",
       "1                                     0   \n",
       "2                                     0   \n",
       "3                                     0   \n",
       "4                                     0   \n",
       "\n",
       "   Largest Property Use Type_Supermarket/Grocery Store  \\\n",
       "0                                                  0     \n",
       "1                                                  0     \n",
       "2                                                  0     \n",
       "3                                                  0     \n",
       "4                                                  0     \n",
       "\n",
       "   Largest Property Use Type_Urgent Care/Clinic/Other Outpatient  \\\n",
       "0                                                  0               \n",
       "1                                                  0               \n",
       "2                                                  0               \n",
       "3                                                  0               \n",
       "4                                                  0               \n",
       "\n",
       "   Largest Property Use Type_Wholesale Club/Supercenter  \\\n",
       "0                                                  0      \n",
       "1                                                  0      \n",
       "2                                                  0      \n",
       "3                                                  0      \n",
       "4                                                  0      \n",
       "\n",
       "   Largest Property Use Type_Worship Facility  \n",
       "0                                           0  \n",
       "1                                           0  \n",
       "2                                           0  \n",
       "3                                           0  \n",
       "4                                           0  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lhbd6f_bKKgb"
   },
   "source": [
    "In the code below, we convert to `numpy` arrays. This is not strictly necessary, but the labels should be converted to a one-dimensional vector (using `reshape` in the code below) or Scikit-Learn will show a warning message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Txom0ypdPE4f"
   },
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "training_features = np.array(train_features)\n",
    "testing_features = np.array(test_features)\n",
    "\n",
    "# Sklearn wants the labels as one-dimensional vectors\n",
    "training_targets = np.array(train_labels).reshape((-1,))\n",
    "testing_targets = np.array(test_labels).reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLUqRxbnLp6h"
   },
   "source": [
    "After the minimal data preparation, we can create the TPOT optimizer. The syntax for [TPOT optimizers](https://epistasislab.github.io/tpot/using/#tpot-with-code) is designed to be as close to that for Scikit-Learn models as possible. \n",
    "\n",
    "The [default parameters for TPOT optimizers](https://epistasislab.github.io/tpot/api/) will test 100 populations of pipelines, each with 100 generations for a total of 10,000 pipelines. Using 10-fold cross validation, this represents 100,000 training runs. Even using Google Colab, this takes quite a while! To avoid running out of time on the Colab server (we get 12 hours of continuous run-time) we will set a maximum of 8 hours (480 minutes) for evaluation.  [TPOT is designed to be run for days](https://epistasislab.github.io/tpot/using/) to thoroughly evaluate many pipelines, but the results can be quite good even from a few hours of training. \n",
    "\n",
    "We set the following parameters in the call to the optimizer (feel free to change these and see how they affect the results):\n",
    "\n",
    "* `scoring = neg_mean_absolute_error`: Our selected regression performance metric\n",
    "* `max_time_mins = 480`: Limit evaluation to 8 hours\n",
    "* `n_jobs = -1`: Use all available cores on the machine\n",
    "* `verbosity = 2`: Show a limited amount of information while training\n",
    "* `cv = 5`: Use 5-fold cross validation (default is 10)\n",
    "\n",
    "After we create the optimizer, we `fit` it to the training data as with any Scikit-Learn machine learning model. This starts the optimization process which will continue for 8 hours. During training, we can see a limited amount of information (change the `verbosity` to see more or less)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g268joSvPGMl"
   },
   "outputs": [],
   "source": [
    "# Create a tpot object with a few parameters\n",
    "tpot = TPOTRegressor(scoring = 'neg_mean_absolute_error', \n",
    "                    max_time_mins = 480, \n",
    "                    n_jobs = -1,\n",
    "                    verbosity = 2,\n",
    "                    cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "colab_type": "code",
    "id": "17SbSKs-Qg2-",
    "outputId": "6fbb2ef4-0ec0-4836-b3f6-1e5b12a2df05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values in feature set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1014279b9a44989f0569f5f4de640e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Optimization Progress'), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -8.969176032882839\n",
      "\n",
      "Generation 2 - Current best internal CV score: -8.79348770260966\n",
      "\n",
      "Generation 3 - Current best internal CV score: -8.79348770260966\n",
      "\n",
      "Generation 4 - Current best internal CV score: -8.75011976828912\n",
      "\n",
      "Generation 5 - Current best internal CV score: -8.749462706374084\n",
      "\n",
      "Generation 6 - Current best internal CV score: -8.630354097317971\n",
      "\n",
      "Generation 7 - Current best internal CV score: -8.630354097317971\n",
      "\n",
      "Generation 8 - Current best internal CV score: -8.630354097317971\n",
      "\n",
      "Generation 9 - Current best internal CV score: -8.541200136350804\n",
      "\n",
      "Generation 10 - Current best internal CV score: -8.500001192658267\n",
      "\n",
      "Generation 11 - Current best internal CV score: -8.500001192658267\n",
      "\n",
      "Generation 12 - Current best internal CV score: -8.500001192658267\n",
      "\n",
      "Generation 13 - Current best internal CV score: -8.500001192658267\n",
      "\n",
      "Generation 14 - Current best internal CV score: -8.500001192658267\n",
      "\n",
      "Generation 15 - Current best internal CV score: -8.500001192658267\n",
      "\n",
      "Generation 16 - Current best internal CV score: -8.488675044586403\n",
      "\n",
      "Generation 17 - Current best internal CV score: -8.488675044586403\n",
      "\n",
      "Generation 18 - Current best internal CV score: -8.466396837081305\n",
      "\n",
      "Generation 19 - Current best internal CV score: -8.466396837081305\n",
      "\n",
      "Generation 20 - Current best internal CV score: -8.454944919129737\n",
      "\n",
      "Generation 21 - Current best internal CV score: -8.454944919129737\n",
      "\n",
      "Generation 22 - Current best internal CV score: -8.454944919129737\n",
      "\n",
      "Generation 23 - Current best internal CV score: -8.454944919129737\n",
      "\n",
      "Generation 24 - Current best internal CV score: -8.454944919129737\n",
      "\n",
      "Generation 25 - Current best internal CV score: -8.454944919129737\n",
      "\n",
      "Generation 26 - Current best internal CV score: -8.454944919129737\n",
      "\n",
      "Generation 27 - Current best internal CV score: -8.445195220990414\n",
      "\n",
      "Generation 28 - Current best internal CV score: -8.445195220990414\n",
      "\n",
      "Generation 29 - Current best internal CV score: -8.445195220990414\n",
      "\n",
      "Generation 30 - Current best internal CV score: -8.437316698873246\n",
      "\n",
      "Generation 31 - Current best internal CV score: -8.435794774040929\n",
      "\n",
      "Generation 32 - Current best internal CV score: -8.435794774040929\n",
      "\n",
      "Generation 33 - Current best internal CV score: -8.435533297517154\n",
      "\n",
      "Generation 34 - Current best internal CV score: -8.435533297517154\n",
      "\n",
      "Generation 35 - Current best internal CV score: -8.423040040348022\n",
      "\n",
      "Generation 36 - Current best internal CV score: -8.423040040348022\n",
      "\n",
      "Generation 37 - Current best internal CV score: -8.423040040348022\n",
      "\n",
      "Generation 38 - Current best internal CV score: -8.423040040348022\n",
      "\n",
      "Generation 39 - Current best internal CV score: -8.423040040348022\n",
      "\n",
      "Generation 40 - Current best internal CV score: -8.423040040348022\n",
      "\n",
      "Generation 41 - Current best internal CV score: -8.423040040348022\n",
      "\n",
      "Generation 42 - Current best internal CV score: -8.419297831537317\n",
      "\n",
      "Generation 43 - Current best internal CV score: -8.414310222655626\n",
      "\n",
      "Generation 44 - Current best internal CV score: -8.414310222655626\n",
      "\n",
      "Generation 45 - Current best internal CV score: -8.40312209746585\n",
      "\n",
      "Generation 46 - Current best internal CV score: -8.40312209746585\n",
      "\n",
      "Generation 47 - Current best internal CV score: -8.40312209746585\n",
      "\n",
      "Generation 48 - Current best internal CV score: -8.40312209746585\n",
      "\n",
      "Generation 49 - Current best internal CV score: -8.40312209746585\n",
      "\n",
      "Generation 50 - Current best internal CV score: -8.400665404978735\n",
      "\n",
      "Generation 51 - Current best internal CV score: -8.398438797443772\n",
      "\n",
      "Generation 52 - Current best internal CV score: -8.395748438858462\n",
      "\n",
      "Generation 53 - Current best internal CV score: -8.307446163334026\n",
      "\n",
      "Generation 54 - Current best internal CV score: -8.307446163334026\n",
      "\n",
      "Generation 55 - Current best internal CV score: -8.307446163334026\n",
      "\n",
      "Generation 56 - Current best internal CV score: -8.307446163334026\n",
      "\n",
      "Generation 57 - Current best internal CV score: -8.307446163334026\n",
      "\n",
      "Generation 58 - Current best internal CV score: -8.307446163334026\n",
      "\n",
      "Generation 59 - Current best internal CV score: -8.307446163334026\n",
      "\n",
      "480.20 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: ExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, learning_rate=0.1, max_depth=6, min_child_weight=2, n_estimators=100, n_jobs=1, objective=reg:squarederror, subsample=0.6500000000000001, verbosity=0), XGBRegressor(RidgeCV(RobustScaler(input_matrix)), learning_rate=0.1, max_depth=6, min_child_weight=3, n_estimators=100, n_jobs=1, objective=reg:squarederror, subsample=0.8500000000000001, verbosity=0)), bootstrap=False, max_features=0.3, min_samples_leaf=2, min_samples_split=19, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTRegressor(max_time_mins=480, n_jobs=-1, scoring='neg_mean_absolute_error',\n",
       "              verbosity=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the tpot model on the training data\n",
    "tpot.fit(training_features, training_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I_Er00uxNa_n"
   },
   "source": [
    "Due to the time limit, we can see our model only was able to get through 15 generations. With 100 populations, this represents 1500 different individual pipelines that were evaluated, quit a few more than we would be able to try by hand! \n",
    "\n",
    "Once the model has finished training, we can see the optimal pipeline by printing the `fitted_pipeline`. This represents the complete pipeline with the best performance metric (in this case the highest `neg_mean_absolute_error`) from cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "1P1r23sqQkbW",
    "outputId": "1cc6f365-43f1-485d-8cfd-fe4b25738451"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('featureunion',\n",
      "                 FeatureUnion(transformer_list=[('stackingestimator-1',\n",
      "                                                 StackingEstimator(estimator=XGBRegressor(base_score=0.5,\n",
      "                                                                                          booster='gbtree',\n",
      "                                                                                          colsample_bylevel=1,\n",
      "                                                                                          colsample_bynode=1,\n",
      "                                                                                          colsample_bytree=1,\n",
      "                                                                                          gamma=0,\n",
      "                                                                                          gpu_id=-1,\n",
      "                                                                                          importance_type='gain',\n",
      "                                                                                          interaction_constraints='',\n",
      "                                                                                          learning_rate=0.1,\n",
      "                                                                                          max_delta_step=0,\n",
      "                                                                                          max_depth=6,\n",
      "                                                                                          min_child_weight=2...\n",
      "                                                                                                           min_child_weight=3,\n",
      "                                                                                                           missing=nan,\n",
      "                                                                                                           monotone_constraints='()',\n",
      "                                                                                                           n_estimators=100,\n",
      "                                                                                                           n_jobs=1,\n",
      "                                                                                                           num_parallel_tree=1,\n",
      "                                                                                                           random_state=0,\n",
      "                                                                                                           reg_alpha=0,\n",
      "                                                                                                           reg_lambda=1,\n",
      "                                                                                                           scale_pos_weight=1,\n",
      "                                                                                                           subsample=0.8500000000000001,\n",
      "                                                                                                           tree_method='exact',\n",
      "                                                                                                           validate_parameters=1,\n",
      "                                                                                                           verbosity=0))])))])),\n",
      "                ('extratreesregressor',\n",
      "                 ExtraTreesRegressor(max_features=0.3, min_samples_leaf=2,\n",
      "                                     min_samples_split=19))])\n"
     ]
    }
   ],
   "source": [
    "# Show the final model\n",
    "print(tpot.fitted_pipeline_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2jwXtE8s4l6w"
   },
   "source": [
    "The TPOT optimization process is stochastic, meaning that [each run will produce different results](https://epistasislab.github.io/tpot/using/). If you run this notebook again, don't worry if you see a different final pipeline!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkSD9-wGNt4R"
   },
   "source": [
    "To save the pipeline for future use, we can export it to a Python script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2c9_uFYYGwB8",
    "outputId": "eed441bf-657b-4fb1-930a-3919df1d9411"
   },
   "outputs": [],
   "source": [
    "# Export the pipeline as a python script file\n",
    "tpot.export('tpot_exported_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vdeqtsImN1WT"
   },
   "source": [
    "Since we are in a Google Colab notebook, this will save it on the server where our notebook is running and the file  will only persist while we are connected. To download the pipeline onto a local machine from Google's servers, we have to use the file helper functions (from `gooogle.colab`) to download it. \n",
    "\n",
    "The file can be [accessed on GitHub](https://github.com/WillKoehrsen/machine-learning-project-walkthrough/blob/master/auto_ml/tpot_exported_pipeline.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1V8P4r4-HgtR"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9dc87455177e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import file management\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Download the pipeline for local use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tpot_exported_pipeline.py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Import file management\n",
    "from google.colab import file\n",
    "\n",
    "# Download the pipeline for local use\n",
    "files.download('tpot_exported_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-y6Qfl58_A_"
   },
   "source": [
    "If we want to look at all of the evaluated pipelines, we can see the `.evaluated_individuals_` attribute of the fitted optimizer. Be careful about running this as it will print out all 1500 pipelines that were tested!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2fkGLxUT9KjK"
   },
   "outputs": [],
   "source": [
    "# To examine all fitted models\n",
    "# tpot.evaluated_individuals_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OQPhr6O5N_7h"
   },
   "source": [
    "Finally, let's test the entire fitted pipeline on the test dataset. After evaluating all the pipelines, TPOT saves the best one and trains it on all the training data, so we can evaluate the best one using the optimizer `.score` method. This will display the negative mean squared error, our regression metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "QAkaMFukQswd",
    "outputId": "26eafbb7-b06c-4e1a-f89e-ff4cec3bc696"
   },
   "outputs": [],
   "source": [
    "# Evaluate the final model\n",
    "print(tpot.score(testing_features, testing_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d0O7CqrTjf8V"
   },
   "source": [
    "In the series of articles where we solved this problem by hand, we built a `GradientBoostedRegressor` model that achieved 9.1 mean absolute error on the test set. Automated machine learning has significantly improved on that score with a drastic reduction in the amount of development time. This \"data science assistant\" feels like the future of data science! \n",
    "\n",
    "Here is the actual implementation of the final pipeline which I copy and pasted from the downloaded Python file. We can train and test it just to make sure that this score is correct! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVM3v_s66A1b"
   },
   "outputs": [],
   "source": [
    "# Imports that the final pipeline needs\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import Imputer\n",
    "from tpot.builtins import StackingEstimator\n",
    "\n",
    "# Preprocessing steps\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "imputer.fit(training_features)\n",
    "training_features = imputer.transform(training_features)\n",
    "testing_features = imputer.transform(testing_features)\n",
    "\n",
    "# Final pipeline from TPOT\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "    GradientBoostingRegressor(alpha=0.95, learning_rate=0.1, loss=\"lad\", \n",
    "                              max_depth=7, max_features=0.75, \n",
    "                              min_samples_leaf=3, min_samples_split=18, \n",
    "                              n_estimators=100, subsample=0.60)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "zR_UtoCY7LWr",
    "outputId": "9d55812f-4cd3-451b-8edd-0c0be794eea0"
   },
   "outputs": [],
   "source": [
    "# Fit on the training data\n",
    "exported_pipeline.fit(training_features, training_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mfge49CA72wc"
   },
   "source": [
    "After creating the optimized pipeline and training it, we can evaluate it on the testing set. As the models were not created with a `random_state`, we expect slightly different performance than the original results, but it should be fairly close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WaA26pr87M_A",
    "outputId": "6a78011e-4157-4e8f-e7be-d6ba700b2506"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the testing data\n",
    "predictions = exported_pipeline.predict(testing_features)\n",
    "\n",
    "print('Mean Absolute Error = %0.4f' % np.mean(abs(predictions - testing_targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mlab54GV8JTa"
   },
   "source": [
    "Sure enough, the mean absolute error is close to that from the optimizer `.score` method and considerably better than our manual pipeline building efforts. \n",
    "\n",
    "From here, we can use the optimization results and try to further fine-tune the pipeline, or we can move on to important phases of the data science workflow. If we use this as the final model, we could spend time trying to intrepret the model (perhaps using [LIME: Local Interpretable Model-Agnostic Explainations](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime)) or reporting our results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tsJ3rNXl9QLg"
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "We saw how to use TPOT, an auto-ml Python library for automatically designing a machine learning pipeline. This tool will evaluate thousands of feature construction, feature selection, model selection, and hyperparameter tuning configurations in order to simplify the job of the data scientist. As machine learning is just one part of the data science workflow, auto-ml will not replace the data scientist, but allow her to spend time on more important aspects of the process. Automated machine learning is still in its early stages, but it appears to be a promising method for optimizing the often tedious and frustrating task of finding the best machine learning pipeline. \n",
    "\n",
    "While being an early adopter does not always pay off, in this case, TPOT is mature enough to have a simple-to-use interface, but also new enough that you will be ahead of the curve if you are familiar with its use. With that in mind, find a problem and get out there are try to solve it! If you are looking for a place to start, [Kaggle](https://www.kaggle.com/) (The Self-Proclaimed Home of Data Science) has many datasets and problems that are well-suited for application of auto-ml. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hvGlKNHMXZOv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "auto_ml_tpot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
